2025-06-06

# The Generative AI Revolution I Witnessed

I've been in the generative AI space since its very first heartbeats. Throughout this journey, I've had the chance to try _every technology_, _every innovation_, _every model_ that came out. I've built countless projects and even contributed to many of them.

## The Early Days: Criticism and Doubt

In the early days, it was criticized for **_inadequate and random outputs_** (language models included). Then came the critiques that it would **never reach human-level performance**. During the period when it couldn't produce sufficiently realistic outputs but succeeded in more abstract arts, there were small pushbacks mixed with mild fear, claiming _"this isn't real art"_ - it was even dismissed. When realism improved, everyone was impressed, but simultaneously there were attempts to build public opinion that **"we should stop AI."**

![](<https://raw.githubusercontent.com/cobanov/blog-renderer/refs/heads/main/assets/TimeToDisco(0)_14.png>)

## Today: Ubiquitous and Indispensable

Today, we've reached a point where it's **_everywhere in our lives_**. It accelerates our work, solves many of our problems, and we can barely distinguish it anymore. But it's certain that it **empowers us** and we don't want to remove it from our lives.

Throughout this entire development process, I noticed one thing: **_no matter what, if a thread starts to unravel, there will always be people who pull it all the way through_**. I was part of that, and I always advocated for it.

## The Relentless Push Forward

During this process, all developers worked both for **better outputs** and to run more efficiently on **lower-end hardware**. No matter what anyone said, someone always kept going and believed. When they hit a wall or reached a limit, they didn't stop - they **_changed the game_** and pushed the boundaries.

> Think [latent diffusion](https://github.com/CompVis/latent-diffusion), [flash attention](https://github.com/Dao-AILab/flash-attention), [low-rank adaptation](https://github.com/microsoft/LoRA), [PEFT](https://github.com/huggingface/peft), AI tooling, [agent MCP](https://github.com/modelcontextprotocol/servers) - the list goes on

## The Innovation Cascade

If you noticed, throughout the entire development cycle, people continuously created new things until they reached **saturation in each respective area**.

We generated images with [_Disco Diffusion_](https://github.com/alembics/disco-diffusion) - low resolution, taking minutes. Latent upscalers came along, and we tried to enlarge those small images. Depth estimation models were invented, we did the first video experiments with _depth warp_ and _optical flow_. [_Stable Diffusion_](https://github.com/Stability-AI/stablediffusion) arrived, prompts got optimized, images became more promising. [_xFormers_](https://github.com/facebookresearch/xformers) came and everything **_accelerated incredibly_**. We saw models that could produce outputs in just **4-5 steps**. We started conditioning those invented depth models with [_ControlNet_](https://github.com/lllyasviel/ControlNet) for style transfer, and so on. We saw the first _text2video_ models, with audio models added in parallel.

## Where We Stand Today

Where we stand today, they've reached **_cinematic quality_** - voices are **nearly impossible** to distinguish. I'm not even talking about language models. AI opposition has vanished, and the limits of _"language models and image models can only do this much"_ have disappeared.

## The Most Beautiful Part

The most beautiful part is that almost everything was done **open source**. In other words, the world's most important technology right now was realized and developed through the **_collective effort of all people worldwide_**.

_That's a spine-tingling feeling, in my opinion._

---

**_By Mert Cobanov on June 6, 2025_**
